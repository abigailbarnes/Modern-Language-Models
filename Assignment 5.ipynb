{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "fdf08401",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "8916f9c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3611967\n"
     ]
    }
   ],
   "source": [
    "from striprtf.striprtf import rtf_to_text\n",
    "\n",
    "'''\n",
    "with open(\"/Users/christopherbarnes/cmsc25910/Assignment5/pulled.rtf\") as infile:\n",
    "    content = infile.read()\n",
    "    text = rtf_to_text(content)\n",
    "#print(text)\n",
    "'''\n",
    "\n",
    "with open('/Users/christopherbarnes/cmsc25910/Assignment5/pulled.txt') as f:\n",
    "    lines = f.readlines()\n",
    "f.close()\n",
    "\n",
    "\n",
    "print(len(lines))\n",
    "#print(lines[0])\n",
    "#print(lines[1])\n",
    "#print(type(lines[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "98bf972f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "\n",
    "# open the file in the write mode\n",
    "f_csv = open('/Users/christopherbarnes/cmsc25910/Assignment5/pulled_csv.csv', 'w')\n",
    "\n",
    "# create the csv writer\n",
    "writer = csv.writer(f_csv)\n",
    "\n",
    "\n",
    "i = 1\n",
    "titles = lines[0].split('\\t')\n",
    "#print(titles)\n",
    "writer.writerow(titles)\n",
    "while i < 101:\n",
    "    rand = random.randint(1, 3611967)\n",
    "    temp = lines[rand].split('\\t')\n",
    "    writer.writerow(temp)\n",
    "    #print(temp)\n",
    "    i += 1\n",
    "\n",
    "# close the file\n",
    "f_csv.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "51e6e771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['california marriages', 'wwww.usps.com', 'golden age card', 'wwwcalexpo.com', 'sguid abdfd2a9f0464977a1722afc3adcc9b9', 'oln.com', 'port-au-prince', 'kylie minogue pics', 'lowndes county alabama emergency response', 'kaplan', 'jackass the movie', 'los angeles california paratransit transportation program', '-', 'bowers cave', 'google.com', 'dummy m-1 garand', 'waist cincher', 'toyota tacoma', 'insert', 'swedenrock festival 2006', 'lee county school board', 'goo', 'shigella', 'free language translating sites', 'red potato casserole', 'http www.1eyed jacks.com', 'map of sunnyside queens ny', 'white bumps in vaginal area after shaving', 'spanking central', 'skillet lasagna recipe', 'san patricios', 'woman clothing', 'vegetable soup recipes', 'meningioma', 'desktop weather', 'udba operativna akcija janjicar', 'www.official trust fuding group.com', 'pregnant galleries', 'family guy porn', 'elvis the ghetto', 'http www.groups.msn.com', 'antique boat motors', 'how much contacts cost', 'ebay', 'esmas.com', '-', 'pictures of amatuer pin up girls', 'casting calls for baby phat models', 'orange county jail inmate locator', 'form 990', 'www.linkinpark.cm', 'keota oldtimers days 2006', 'camel toes', 'california coast', 'tattoos', 'my bange me', 'wlbt.com', 'free tarot readings', 'gmail', 'numanumadance', 'judy jakes ministries', 'tree silhouettes', \"church's banner\", 'silver sage thermogenics ephedrea', 'the antisceptic baby and the prophylactic pup', 'ebay', 'goldenrold paper', 'mp3 downloads', 'www.dhmco.co', 'the sims game', 'unicare insurance nevada', 'thunder valley casino', '.aldappd em0046545503 pre-delivery alert summary page 3492date run 3 15 2006 chicopaton aol.com time 18 09this message is to alert you that the following package s is are duefor delivery by fedex', 'erika elaniak', 'camping at big sycamore', 'north hennepin', 'funeral', 'christmasatthemall.com', 'marylandballetyouth.vom', 'www.daysinhotel.com', 'tulane hospital', 'goolge', 'singleme', 'berkeley blood test', 'http party poker.com', 'map guest', 'rossi combo w 3 barrels 270 17hmr 50cal muz new', 'suny sccc', 'www.maxwellt0p100.com', '1973 katharine hepburn movie', 'sharron', 'underage girls nude', 'sturm ruger', 'naked crucifixion', 'xanga.com', 'chuck norris jokes', 'tyngsboro school', 'rajashwini.com', 'us postmaster moving', 'clarence and louise stout rochester ny']\n",
      "['10', '', '3', '', '', '2', '', '', '4', '2', '', '4', '', '1', '1', '2', '2', '', '', '2', '1', '', '3', '16', '', '', '1', '3', '1', '7', '4', '15', '4', '', '1', '', '', '', '7', '5', '', '', '', '1', '2', '4', '', '1', '1', '13', '', '', '', '', '', '', '1', '', '1', '', '', '', '', '', '', '', '1', '', '', '7', '', '1', '', '1', '17', '1', '', '7', '1', '', '3', '', '', '1', '', '', '', '', '', '', '', '8', '', '', '2', '1', '', '', '1', '']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "queries = []\n",
    "clicks = []\n",
    "\n",
    "with open('/Users/christopherbarnes/cmsc25910/Assignment5/csv_to_be_redacted.csv') as file_obj:\n",
    "      \n",
    "    # Create reader object by passing the file \n",
    "    # object to reader method\n",
    "    reader_obj = csv.reader(file_obj)\n",
    "      \n",
    "    # Iterate over each row in the csv \n",
    "    # file using reader object\n",
    "    for row in reader_obj:\n",
    "        queries.append(row[1])\n",
    "        clicks.append(row[3])\n",
    "\n",
    "queries.remove('Query')\n",
    "print(queries)\n",
    "clicks.remove('ItemRank')\n",
    "print(clicks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "580b3e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 4, 2]\n"
     ]
    }
   ],
   "source": [
    "#1.\tNon-Trivial: Number of Capitalized characters for each search query\n",
    "#I am realizing this probably isnt the most helpful given the data, but I am going to make up my own little list\n",
    "#to make this work\n",
    "\n",
    "test_queries = ['hi my name is Abby Barnes and I enjoy Computer Science. Even if it is hard, sometimes', 'bro I dont. know. why. I. Chose. This.', 'Atlanta, Georgia is the best city on earth!']\n",
    "\n",
    "def capitalized(q):\n",
    "    ans = []\n",
    "    for query in q:\n",
    "        counter = 0\n",
    "        splt = query.split(' ')\n",
    "        for word in splt:\n",
    "            if word[0].isupper():\n",
    "                counter += 1\n",
    "        ans.append(counter)\n",
    "    return ans\n",
    "\n",
    "print(capitalized(test_queries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e4d1a67b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "#2.\tNon-Trivial: Number of rows whose columns contain missing data\n",
    "#After considering the output, this could also be considered a binary output indicating whether or not the \n",
    "#query is missing or not since the 1 values correlate to where there is, in fact, missing query information \n",
    "\n",
    "def nan_count(q):\n",
    "    ans = []\n",
    "    for query in q:\n",
    "        counter = 0\n",
    "        if '-' == query:\n",
    "            counter += 1\n",
    "        ans.append(counter)\n",
    "    return ans\n",
    "        \n",
    "print(nan_count(queries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "6b8854b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10', '', '3', '', '', '2', '', '', '4', '2', '', '4', '', '1', '1', '2', '2', '', '', '2', '1', '', '3', '16', '', '', '1', '3', '1', '7', '4', '15', '4', '', '1', '', '', '', '7', '5', '', '', '', '1', '2', '4', '', '1', '1', '13', '', '', '', '', '', '', '1', '', '1', '', '', '', '', '', '', '', '1', '', '', '7', '', '1', '', '1', '17', '1', '', '7', '1', '', '3', '', '', '1', '', '', '', '', '', '', '', '8', '', '', '2', '1', '', '', '1', '']\n",
      "['10-14', 'less than 1', '1-4', 'less than 1', 'less than 1', '1-4', 'less than 1', 'less than 1', '1-4', '1-4', 'less than 1', '1-4', 'less than 1', '1-4', '1-4', '1-4', '1-4', 'less than 1', 'less than 1', '1-4', '1-4', 'less than 1', '1-4', '15 or more', 'less than 1', 'less than 1', '1-4', '1-4', '1-4', '5-9', '1-4', '15 or more', '1-4', 'less than 1', '1-4', 'less than 1', 'less than 1', 'less than 1', '5-9', '5-9', 'less than 1', 'less than 1', 'less than 1', '1-4', '1-4', '1-4', 'less than 1', '1-4', '1-4', '10-14', 'less than 1', 'less than 1', 'less than 1', 'less than 1', 'less than 1', 'less than 1', '1-4', 'less than 1', '1-4', 'less than 1', 'less than 1', 'less than 1', 'less than 1', 'less than 1', 'less than 1', 'less than 1', '1-4', 'less than 1', 'less than 1', '5-9', 'less than 1', '1-4', 'less than 1', '1-4', '15 or more', '1-4', 'less than 1', '5-9', '1-4', 'less than 1', '1-4', 'less than 1', 'less than 1', '1-4', 'less than 1', 'less than 1', 'less than 1', 'less than 1', 'less than 1', 'less than 1', 'less than 1', '5-9', 'less than 1', 'less than 1', '1-4', '1-4', 'less than 1', 'less than 1', '1-4', 'less than 1']\n"
     ]
    }
   ],
   "source": [
    "#3.\tNon-Trivial: Binning values from the “clicks” column\n",
    "print(clicks)\n",
    "def click_binning(c):\n",
    "    ans = []\n",
    "    for click in c:\n",
    "        if '' == click:\n",
    "            ans.append('less than 1')\n",
    "        elif int(click) > 0 and int(click) < 5:\n",
    "            ans.append('1-4')\n",
    "        elif int(click) >= 5 and int(click) < 10:\n",
    "            ans.append('5-9')\n",
    "        elif int(click) >= 10 and int(click) < 15:\n",
    "            ans.append('10-14')\n",
    "        else:\n",
    "            ans.append('15 or more')\n",
    "    return ans\n",
    "\n",
    "print(click_binning(clicks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c2bb899e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1124)>\n",
      "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1124)>\n",
      "[nltk_data] Error loading corpus: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1124)>\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/christopherbarnes/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.8/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.8/share/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.8/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [108]\u001b[0m, in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m         stopwords_x \u001b[38;5;241m=\u001b[39m [w \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m word_tokens \u001b[38;5;28;01mif\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m stop_words]\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(stopwords_x)\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mcount_stopwords\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqueries\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Input \u001b[0;32mIn [108]\u001b[0m, in \u001b[0;36mcount_stopwords\u001b[0;34m(q)\u001b[0m\n\u001b[1;32m     11\u001b[0m stop_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(stopwords\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m))  \n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m query \u001b[38;5;129;01min\u001b[39;00m q:\n\u001b[0;32m---> 13\u001b[0m     word_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     stopwords_x \u001b[38;5;241m=\u001b[39m [w \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m word_tokens \u001b[38;5;28;01mif\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m stop_words]\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(stopwords_x)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/nltk/tokenize/__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    131\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    132\u001b[0m     ]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/nltk/tokenize/__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlanguage\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pickle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/nltk/data.py:750\u001b[0m, in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    749\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[0;32m--> 750\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m \u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/nltk/data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    873\u001b[0m protocol, path_ \u001b[38;5;241m=\u001b[39m split_resource_url(resource_url)\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mopen()\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    878\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[1;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/christopherbarnes/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.8/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.8/share/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.8/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "#4.\tNon-Trivial: Number of stop words\n",
    "'''\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('corpus')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def count_stopwords(q):\n",
    "    stop_words = set(stopwords.words('english'))  \n",
    "    for query in q:\n",
    "        word_tokens = word_tokenize(query)\n",
    "        stopwords_x = [w for w in word_tokens if w in stop_words]\n",
    "    return len(stopwords_x)\n",
    "\n",
    "print(count_stopwords(queries))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "46d38fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 1, 1, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "#4. Number of Profane words: Changed from write up due to import issues\n",
    "#for the sake of this feature, I created an example set of queries that is not related to the data pulled from the\n",
    "#given website from the writeup\n",
    "from better_profanity import profanity\n",
    "\n",
    "queries_example = ['fuck you!', 'walruses are phenomenal', 'stupid', 'crap!', 'i am typing code!', '>:(', 'i am listening to the harry potter audio book... draco malfo is a piece of shit']\n",
    "\n",
    "def count_profanity(q):\n",
    "    arr = []\n",
    "    for query in q:\n",
    "        count = 0\n",
    "        split = query.split(' ')\n",
    "        for word in split:\n",
    "            #print(word)\n",
    "            if profanity.contains_profanity(word):\n",
    "                count += 1\n",
    "        arr.append(count)\n",
    "    return arr\n",
    "\n",
    "print(count_profanity(queries_example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "f9cd799d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "#5.\tNon-Trivial: Number of punctuation marks\n",
    "import re\n",
    "\n",
    "s = \"string. With. Punctuation?\"\n",
    "out = re.sub(r'[^\\w\\s]','',s)\n",
    "\n",
    "\n",
    "def count_punctuations(q):\n",
    "    ans = []\n",
    "    punctuations='!#$%&()*+,-./:;<=>?@[\\]^_`{|}~'\n",
    "    for query in q:\n",
    "        counter = 0\n",
    "        for p in punctuations:\n",
    "            if p in query:\n",
    "                counter += 1\n",
    "        ans.append(counter)\n",
    "    return ans\n",
    "\n",
    "print(count_punctuations(queries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "467ac951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "#6.\tTrivial: Checking for number of states (name) in each dataset\n",
    "\n",
    "states = ['Alaska','Alabama','Arkansas','Arizona','California','Colorado','Connecticut','District of Columbia','Delaware','Florida','Georgia','Hawaii','Iowa','Idaho','Illinois','Indiana','Kansas','Kentucky','Louisiana','Massachusetts','Maryland','Maine','Michigan','Minnesota','Missouri','Mississippi', 'Montana','North Carolina','North Dakota','Nebraska','New Hampshire','New Jersey','New Mexico','Nevada','New York','Ohio', 'Oklahoma', 'Oregon','Pennsylvania','Rhode Island','South Carolina','South Dakota','Tennessee','Texas', 'Utah', 'Virginia', 'Vermont', 'Washington', 'Wisconsin', 'West Virginia', 'Wyoming']\n",
    "\n",
    "def num_states(q):\n",
    "    ans = []\n",
    "    for query in q:\n",
    "        count = 0\n",
    "        for s in states:\n",
    "            if s.lower() in query.lower():\n",
    "                count += 1\n",
    "        ans.append(count)\n",
    "    return ans\n",
    "\n",
    "print(num_states(queries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "ca2b5a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9.5, 13.0, 4.333333333333333, 14.0, 18.5, 7.0, 14.0, 5.333333333333333, 7.4, 6.0, 5.0, 8.666666666666666, 1.0, 5.0, 10.0, 4.666666666666667, 6.0, 6.0, 6.0, 7.333333333333333, 5.0, 3.0, 8.0, 7.0, 6.0, 7.333333333333333, 4.4, 5.0, 7.5, 6.666666666666667, 6.0, 6.5, 6.666666666666667, 10.0, 7.0, 7.0, 8.0, 8.5, 4.333333333333333, 4.666666666666667, 11.0, 5.666666666666667, 4.75, 4.0, 9.0, 1.0, 4.5, 4.833333333333333, 5.8, 3.5, 17.0, 5.5, 4.5, 7.5, 7.0, 3.0, 8.0, 5.666666666666667, 5.0, 13.0, 6.333333333333333, 7.5, 7.0, 7.5, 5.571428571428571, 4.0, 7.5, 6.0, 12.0, 3.6666666666666665, 7.333333333333333, 6.333333333333333, 5.125, 6.0, 5.0, 6.5, 7.0, 22.0, 23.0, 19.0, 7.0, 6.0, 8.0, 5.666666666666667, 6.0, 4.0, 3.8, 4.0, 21.0, 6.25, 7.0, 5.666666666666667, 5.0, 8.0, 9.0, 5.333333333333333, 7.5, 14.0, 6.0, 5.5]\n"
     ]
    }
   ],
   "source": [
    "#7.\tTrivial: Average word length\n",
    "def average_word_length(q):\n",
    "    ans = []\n",
    "    for query in q:\n",
    "        s = 0\n",
    "        split = query.split(\" \")\n",
    "        for word in split:\n",
    "            s += len(word)\n",
    "        ans.append(s / len(split))\n",
    "    return ans\n",
    "\n",
    "print(average_word_length(queries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "6696ac68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 1, 3, 1, 2, 1, 1, 3, 5, 1, 3, 6, 1, 2, 1, 3, 2, 2, 1, 3, 4, 1, 1, 4, 3, 3, 5, 7, 2, 3, 2, 2, 3, 1, 2, 4, 4, 2, 3, 3, 2, 3, 4, 1, 1, 1, 6, 6, 5, 2, 1, 4, 2, 2, 1, 3, 1, 3, 1, 1, 3, 2, 2, 4, 7, 1, 2, 2, 1, 3, 3, 3, 32, 2, 4, 2, 1, 1, 1, 1, 2, 1, 1, 3, 3, 2, 10, 2, 1, 4, 1, 3, 2, 2, 1, 3, 2, 1, 3, 6]\n"
     ]
    }
   ],
   "source": [
    "#8.\tTrivial: Number of words in a sentence\n",
    "def sentence_length(q):\n",
    "    ans = []\n",
    "    for query in q:\n",
    "        split = query.split(\" \")\n",
    "        ans.append(len(split))\n",
    "    return ans\n",
    "\n",
    "print(sentence_length(queries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "ab90da2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AnonID', 'Query', 'QueryTime', 'ItemRank', 'ClickURL', 'Redacted']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AnonID</th>\n",
       "      <th>Query</th>\n",
       "      <th>QueryTime</th>\n",
       "      <th>ItemRank</th>\n",
       "      <th>ClickURL</th>\n",
       "      <th>Redacted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10798948</td>\n",
       "      <td>california marriages</td>\n",
       "      <td>2006-03-07 23:22:17</td>\n",
       "      <td>10</td>\n",
       "      <td>http://www.rootsweb.com\\n</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3673754</td>\n",
       "      <td>wwww.usps.com</td>\n",
       "      <td>2006-05-26 14:01:59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\n</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17280357</td>\n",
       "      <td>golden age card</td>\n",
       "      <td>2006-03-27 11:50:27</td>\n",
       "      <td>3</td>\n",
       "      <td>http://www.fs.fed.us\\n</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15462250</td>\n",
       "      <td>wwwcalexpo.com</td>\n",
       "      <td>2006-05-12 17:14:27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\n</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4616854</td>\n",
       "      <td>sguid abdfd2a9f0464977a1722afc3adcc9b9</td>\n",
       "      <td>2006-04-04 12:42:06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\n</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>4927596</td>\n",
       "      <td>chuck norris jokes</td>\n",
       "      <td>2006-03-24 15:54:51</td>\n",
       "      <td>1</td>\n",
       "      <td>http://en.wikipedia.org\\n</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>2327118</td>\n",
       "      <td>tyngsboro school</td>\n",
       "      <td>2006-03-02 18:17:04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\n</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>12622450</td>\n",
       "      <td>rajashwini.com</td>\n",
       "      <td>2006-05-14 14:24:49</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\n</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>4999176</td>\n",
       "      <td>us postmaster moving</td>\n",
       "      <td>2006-03-23 15:45:29</td>\n",
       "      <td>1</td>\n",
       "      <td>http://www.usps.com\\n</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>6185322</td>\n",
       "      <td>clarence and louise stout rochester ny</td>\n",
       "      <td>2006-04-18 10:31:15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\n</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       AnonID                                   Query            QueryTime  \\\n",
       "1    10798948                    california marriages  2006-03-07 23:22:17   \n",
       "2     3673754                           wwww.usps.com  2006-05-26 14:01:59   \n",
       "3    17280357                         golden age card  2006-03-27 11:50:27   \n",
       "4    15462250                          wwwcalexpo.com  2006-05-12 17:14:27   \n",
       "5     4616854  sguid abdfd2a9f0464977a1722afc3adcc9b9  2006-04-04 12:42:06   \n",
       "..        ...                                     ...                  ...   \n",
       "96    4927596                      chuck norris jokes  2006-03-24 15:54:51   \n",
       "97    2327118                        tyngsboro school  2006-03-02 18:17:04   \n",
       "98   12622450                          rajashwini.com  2006-05-14 14:24:49   \n",
       "99    4999176                    us postmaster moving  2006-03-23 15:45:29   \n",
       "100   6185322  clarence and louise stout rochester ny  2006-04-18 10:31:15   \n",
       "\n",
       "    ItemRank                   ClickURL Redacted  \n",
       "1         10  http://www.rootsweb.com\\n        1  \n",
       "2        NaN                         \\n        0  \n",
       "3          3     http://www.fs.fed.us\\n        1  \n",
       "4        NaN                         \\n        0  \n",
       "5        NaN                         \\n        0  \n",
       "..       ...                        ...      ...  \n",
       "96         1  http://en.wikipedia.org\\n        0  \n",
       "97       NaN                         \\n        1  \n",
       "98       NaN                         \\n        0  \n",
       "99         1      http://www.usps.com\\n        0  \n",
       "100      NaN                         \\n        1  \n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training Logistic Regression Classifer\n",
    "#import pandas\n",
    "import pandas as pd\n",
    "col_names = ['AnonID', 'Query', 'QueryTime', 'ItemRank', 'ClickURL', 'Redacted']\n",
    "print(col_names)\n",
    "# load dataset\n",
    "pima = pd.read_csv(\"/Users/christopherbarnes/cmsc25910/Assignment5/pulled_csv_redacted.csv\", header=None, names=col_names)\n",
    "pima.drop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a1bff1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "a7d498b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from better_profanity import profanity\n",
    "\n",
    "\n",
    "arr = []\n",
    "for val in pima['Query']:\n",
    "    #print(val)\n",
    "    if profanity.contains_profanity(val):\n",
    "        arr.append(1)\n",
    "    else:\n",
    "        arr.append(0)\n",
    "\n",
    "pima['Profane'] = arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "77afe160",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split dataset in features and target variable\n",
    "#feature_cols = ['AnonID', 'Query', 'QueryTime', 'ItemRank', 'ClickURL', 'Profane']\n",
    "feature_cols = ['Profane']\n",
    "X = pima[feature_cols] # Features\n",
    "y = pima['Redacted'] # Target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "f54b444c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split X and y into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.25,random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "c380f083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the class\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# instantiate the model (using the default parameters)\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# fit the model with data\n",
    "logreg.fit(X_train,y_train)\n",
    "\n",
    "#\n",
    "y_pred1=logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "9bea4328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic regression confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[17,  0],\n",
       "       [ 7,  2]])"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the metrics class\n",
    "from sklearn import metrics\n",
    "cnf_matrix = metrics.confusion_matrix(y_test, y_pred1)\n",
    "print('logistic regression confusion matrix:')\n",
    "cnf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "a41cf55f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using Logistic Regression Model: 0.7307692307692307\n",
      "Precision using Logistic Regression Model: 0.8092948717948718\n",
      "Recall using Logistic Regression Model: 0.7307692307692307\n"
     ]
    }
   ],
   "source": [
    "#Write up Question 3\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "\n",
    "# Model Accuracy: how often is the classifier correct?\n",
    "print(\"Accuracy using Logistic Regression Model:\",metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Model Precision: what percentage of positive tuples are labeled as such?\n",
    "print(\"Precision using Logistic Regression Model:\",metrics.precision_score(y_test, y_pred, average='weighted'))\n",
    "\n",
    "# Model Recall: what percentage of positive tuples are labelled as such?\n",
    "print(\"Recall using Logistic Regression Model:\",metrics.recall_score(y_test, y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "9ef252a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import svm model\n",
    "from sklearn import svm\n",
    "\n",
    "#Create a svm Classifier\n",
    "clf = svm.SVC(kernel='linear') # Linear Kernel\n",
    "\n",
    "#Train the model using the training sets\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred2 = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "bc5f5faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Kernel SVM confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[17,  0],\n",
       "       [ 7,  2]])"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "svm_matrix = metrics.confusion_matrix(y_test, y_pred2)\n",
    "print('Linear Kernel SVM confusion matrix:')\n",
    "svm_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "16868d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using Linear Kernel SVM: 0.7307692307692307\n",
      "Precision using Linear Kernel SVM: 0.8092948717948718\n",
      "Recall using Linear Kernel SVM: 0.7307692307692307\n"
     ]
    }
   ],
   "source": [
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "\n",
    "# Model Accuracy: how often is the classifier correct?\n",
    "print(\"Accuracy using Linear Kernel SVM:\",metrics.accuracy_score(y_test, y_pred2))\n",
    "\n",
    "# Model Precision: what percentage of positive tuples are labeled as such?\n",
    "print(\"Precision using Linear Kernel SVM:\",metrics.precision_score(y_test, y_pred2, average='weighted'))\n",
    "\n",
    "# Model Recall: what percentage of positive tuples are labelled as such?\n",
    "print(\"Recall using Linear Kernel SVM:\",metrics.recall_score(y_test, y_pred2, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "77186832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0' '0' '0' '0' '0' '1' '1' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
      " '0' '0' '0' '0' '0' '0' '0' '0']\n",
      "['0' '0' '0' '0' '0' '1' '1' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
      " '0' '0' '0' '0' '0' '0' '0' '0']\n"
     ]
    }
   ],
   "source": [
    "print(y_pred1)\n",
    "print(y_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8851eb45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
